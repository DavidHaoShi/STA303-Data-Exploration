---
output:
  html_document:
    df_print: paged
  pdf_document: default
urlcolor: blue
header-includes:
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO, CE]{Hao Shi, 1004942440}
- \fancyfoot[CO, CE]{\thepage \ of \pageref{LastPage}}
---

### Delete this section once you've followed these instructions

1. Change 'Your name, your ID' in line 9 above to be your name and ID. No quotes needed.
2. Run the `setup` and `getdata` chunks below. (You can click the green play button at the top right of these chunks.)
3. Click Knit to test that you can run correctly knit this file.
4. Delete this section, up to the first code chunk. I.e. delete the header, "Delete this section once you've followed these instructions", and points 1 through 4. *Don't* delete the `setup` code chunk.

```{r setup, message = FALSE, echo=FALSE}
# Students: You probably shouldn't change any of the code in this chunk.

# These are the packages you will need for this activity
packages_needed <- c("tidyverse", "googledrive", "readxl", "janitor", 
                     "lubridate", "opendatatoronto", "ggthemes")

package.check <- lapply(
  packages_needed,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
    }
  }
)

# Credit: package.check based on a helpful post from Vikram Baliga https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/

# Load tidyverse
library(tidyverse)
library(readxl)
library(janitor)
library(opendatatoronto)
library(ggthemes)

# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), echo = FALSE)
```


```{r getdata, eval = FALSE, echo=FALSE}
# Students: You probably shouldn't change any of the code in this chunk BUT...

# This chunk loads the most recent data from Toronto City and the data from OpenToronto.

# You have to RUN this chunk by hand to update the data as 
#   eval is set to FALSE to limit unnecessary requsts on the site.

###################################################
# Step one: Get the COVID data from Toronto City. #
###################################################

googledrive::drive_deauth()

#url1 <- "https://drive.google.com/file/d/11KF1DuN5tntugNc10ogQDzFnW05ruzLH/view"
#googledrive::drive_download(url1, path="/Users/david/Desktop/STA303/CityofToronto_COVID-19_Daily_Public_Reporting.xlsx", overwrite = TRUE)

url2 <- "https://drive.google.com/file/d/1jzH64LvFQ-UsDibXO0MOtvjbL2CvnV3N/view"
googledrive::drive_download(url2, path = "/Users/david/Desktop/STA303/CityofToronto_COVID-19_NeighbourhoodData.xlsx", overwrite = TRUE)

# this removes the url object that we don't need anymore
rm(url1, url2)

#####################################################################
# Step two: Get the data neighbourhood data from Open Data Toronto. #
#####################################################################

nbhoods_shape_raw <- list_package_resources("neighbourhoods") %>% 
  get_resource()

saveRDS(nbhoods_shape_raw, "/Users/david/Desktop/STA303/neighbourhood_shapefile.Rds")

nbhood_profile <- search_packages("Neighbourhood Profile") %>%
  list_package_resources() %>% 
  filter(name == "neighbourhood-profiles-2016-csv") %>% 
  get_resource()

saveRDS(nbhood_profile, "/Users/david/Desktop/STA303/neighbourhood_profile.Rds")
```


```{r load_data, echo=FALSE}
######################################################
# Step three: Load the COVID data from Toronto City. #
######################################################

# Saving the name of the file as an object and then using the object name in the
# following code is a helpful practice. Why? If we change the name of the file 
# being used, we'll only have to change it in one place. This helps us avoid 
# 'human error'.

daily_data <- "/Users/david/Desktop/STA303/CityofToronto_COVID-19_Daily_Public_Reporting.xlsx"

# Cases reported by date
reported_raw <- read_excel(daily_data, sheet = 5) %>% 
  clean_names()

# Cases by outbreak type
outbreak_raw <- read_excel(daily_data, sheet = 3) %>% 
  clean_names()

# When was this data updated?
date_daily <- read_excel(daily_data, sheet = 1) %>% 
  clean_names()

# By neighbourhood
neighbourood_data <- "/Users/david/Desktop/STA303/CityofToronto_COVID-19_NeighbourhoodData.xlsx"

# Cases reported by date
nbhood_raw <- read_excel(neighbourood_data, sheet = 2) %>% 
  clean_names()

# Date the neighbourhood data was last updated
date_nbhood <- read_excel(neighbourood_data, sheet = 1) %>% 
  clean_names()

#don't need these anymore
rm(daily_data, neighbourood_data)

#############################################################
# Step four: Load the neighbourhood data from Toronto City. #
#############################################################

# Get neighbourhood profile data
nbhood_profile <- readRDS("/Users/david/Desktop/STA303/neighbourhood_profile.Rds")

# Get shape data for mapping 
nbhoods_shape_raw <- readRDS("/Users/david/Desktop/STA303/neighbourhood_shapefile.Rds") %>% 
  sf::st_as_sf() ## Makes sure shape info is in the most up to date format

```

Code last run `r Sys.Date()`.  
Daily: `r date_daily[1,1]`.   
Neighbourhood: `r date_nbhood[1,1]`. 

# Task 1: Daily cases
## Data wrangling

```{r cases_dw,warning=FALSE,echo=TRUE}

reported <- reported_raw %>% 
  mutate_if(is.numeric, replace_na, replace = 0) %>% 
  rename(Recovered = recovered, Active = active, Deceased = deceased) %>%
  pivot_longer(-c(reported_date), names_to = "Status", values_to = "Cases") %>%
  mutate(reported_date=date(reported_date)) 
  
  #This factor() function changes the level of the Status to fit the order of the graph
  reported$Status<-factor(reported$Status,levels=c("Active","Recovered","Deceased")) 


```

\newpage
## Data visualization

```{r cases_vis,warning=FALSE,echo=TRUE}
ggplot(reported,aes(x = reported_date,y= Cases,fill = Status))  + 
  geom_bar(position="stack", stat="identity") + 
  labs(title = "Cases reported by day in Toronto, Canada",
       subtitle = "Confirmed and probable cases",
       x = "Date",
       y = "Case count", 
       caption = str_c("Created by: Hao Shi for STA303/1002, U of T\n", 
                       "Source: Ontario Ministry of Health, Integrated Public Health Information System and CORES\n" ,date_daily[1,1])) + 
  scale_y_continuous(limits=c(0,2000)) + 
  scale_x_date(labels = scales::date_format("%d %b %y"),limits=c(date("2020-01-01"),Sys.Date())) + 
  scale_fill_manual(values =c("#003F5C","#86BCB6","#B9CA5D")) + 
  theme_minimal() +
  theme(legend.title = element_blank(), legend.position = c(0.15, 0.8))
```

\newpage
# Task 2: Outbreak type
## Data wrangling


```{r outbreak_dw,echo=TRUE}
outbreak <- outbreak_raw %>% 
  group_by(episode_week) %>%
  mutate(total_cases= sum(cases)) %>% 
  mutate(episode_week=date(episode_week)) %>% 
  mutate(outbreak_or_sporadic = case_when(
    str_detect(outbreak_or_sporadic, "OB Associated") ~ "Outbreak associated",
    str_detect(outbreak_or_sporadic, "Sporadic") ~ "Sporadic"))


  #This factor() function changes the level of the Status to fit the order of the graph
  outbreak$outbreak_or_sporadic<-factor(outbreak$outbreak_or_sporadic,levels=c("Sporadic","Outbreak associated"))

```

\newpage
## Data visualization

```{r outbreak_vis,warning=FALSE,echo=TRUE}
ggplot(outbreak,aes(x = episode_week,y= cases,fill = outbreak_or_sporadic))  + 
  geom_bar(position="stack", stat="identity") + 
  labs(title = "Cases by outbreak type and week in Toronto, Canada",
       subtitle = "Confirmed and probable cases",
       x = "Date",
       y = "Case count", 
       caption = str_c("Created by: Hao Shi for STA303/1002, U of T\n", 
                       "Source: Ontario Ministry of Health, Integrated Public Health Information System and CORES\n" ,date_daily[1,1])) + 
  scale_y_continuous(limits=c(0,6000)) + 
  scale_x_date(labels = scales::date_format("%d %b %y"),limits=c(date("2020-01-01"),Sys.Date()+7)) + 
  scale_fill_manual(values =c("#86BCB6","#B9CA5D")) + 
  theme_minimal() +
  theme(legend.title = element_blank(), legend.position = c(0.15, 0.8))
```

\newpage
# Task 3: Neighbourhoods
## Data wrangling: part 1

```{r nbhood_dw_1,echo=TRUE}

#This removes the leading space from the observations in the Characteristic colomn
nbhood_profile$Characteristic <- trimws(nbhood_profile$Characteristic, which = c("left"))

income <- nbhood_profile %>% 
  filter(nbhood_profile$Topic=="Low income in 2015",
         nbhood_profile$Characteristic=="18 to 64 years (%)") %>% 
  #We see two matching observations after the filtering
  #We choose 1143 over 1075 due to how the data is structured
  filter(`_id`==1143) %>% 
  select(-c(2:5)) %>%
  pivot_longer(-c(`_id`), names_to = "neighbourhood_name", 
               values_to = "percentage_of_low_income") %>%
  #We then remove the _id colomn
  select(-`_id`)

#We remove the row about city of Toronto since we are interested in neighbourhoods only 
income<-income[-1,]

#This makes sure that percentages are stored as numbers, not character strings
income$percentage_of_low_income<-as.numeric(income$percentage_of_low_income)


```

## Data wrangling: part 2

```{r nbhood_dw_2,echo=TRUE}
nbhoods_all <- nbhoods_shape_raw %>%
  mutate(neighbourhood_name = str_replace(AREA_NAME, "\\s\\(\\d+\\)$","")) %>%
  full_join(nbhood_raw) %>%
  full_join(income) %>%
  select(-neighbourhood_id,-case_count,-PARENT_AREA_ID,
         -X,-Y,-LONGITUDE,-LATITUDE) %>%
  rename(rate_per_100000 = rate_per_100_000_people) #%>%
  #filter(is.na(percentage_of_low_income)==FALSE) %>%
  #filter(is.na(rate_per_100000)==FALSE) %>%

```

## Data wrangling: part 3

```{r nbhood_dw_3,echo=TRUE}
nbhoods_final <- nbhoods_all %>%
  mutate(med_inc = median(nbhoods_all$percentage_of_low_income)) %>%
  mutate(med_rate = median(nbhoods_all$rate_per_100000)) 

nbhoods_final <- nbhoods_final %>%
  mutate(nbhood_type = case_when(
    percentage_of_low_income >= med_inc ~ ifelse(rate_per_100000 >= med_rate,"Higher low income rate, higher case rate", "Higher low income rate, lower case rate"),
    percentage_of_low_income < med_inc ~ ifelse(rate_per_100000 >= med_rate,"Lower low income rate, higher case rate", "Lower low income rate, lower case rate")
    ))

```

\newpage
## Data visualization

```{r neighbourhood_graphs_1, fig.height=4,echo=TRUE}
ggplot(data = nbhoods_final) + 
  geom_sf(aes(fill = percentage_of_low_income)) + 
  theme_map() + 
  scale_fill_gradient(name= "% low income", low = "darkgreen", high = "lightgrey") + 
  labs(title = "Percentage of 18 to 64 year olds living in a low income family (2015)",
       subtitle = "Neighbourhoods of Toronto, Canada",
       caption = str_c("Created by: Hao Shi for STA303/1002, U of T\n", 
                       "Source: Census Profile 98−316−X2016001 via OpenData Toronto\n" ,date_daily[1,1])) + theme(legend.position = "right")
```

\newpage

```{r neighbourhood_graphs_2, fig.height=4,echo=TRUE}

ggplot(data = nbhoods_final) + 
  geom_sf(aes(fill = rate_per_100000)) + 
  theme_map() + 
  scale_fill_gradient(name= "Cases per 100,000 people", low = "white", high = "darkorange") + 
  labs(title = "COVID−19 cases per 100,000, by neighbourhood in Toronto, Canada",
       caption = str_c("Created by: Hao Shi for STA303/1002, U of T\n", 
                       "Source: Ontario Ministry of Health, Integrated Public Health Information System and CORES\n" ,date_daily[1,1])) + theme(legend.position = "right")
```

\newpage

```{r neighbourhood_graphs_3, fig.height=4,warning = FALSE,echo=TRUE}
ggplot(data = nbhoods_final) + 
  geom_sf(aes(fill = nbhood_type)) + 
  scale_fill_discrete(name= "% of 18 to 64 year−olds in\n",
                      "low income families and\n"," COVID−19 case rates") +
  theme_map() + 
  labs(title = "COVID−19 cases per 100,000, by neighbourhood in Toronto, Canada",
       caption = str_c("Created by: Hao Shi for STA303/1002, U of T\n", 
                       "Income data source: Census Profile 98−316−X2016001 via OpenData Toronto\n","COVID data source: Ontario Ministry of Health, Integrated Public\n",
                       "Health Information System and CORES\n",date_daily[1,1])) + 
  theme(legend.position = "right") + scale_fill_brewer(palette = "Set1")
```




```{r, eval = FALSE}
# This chunk of code helps you prepare your assessment for submission on Crowdmark
# This is optional. If it isn't working, you can do it manually/take another approach.

# Run this chunk by hand after knitting your final version of your pdf for submission.
# A new file called 'to_submit' will appear in your working directory with each page of your assignment as a separate pdf.

# Install the required packages
if(!match("staplr", installed.packages()[,1], nomatch = FALSE))
  {install.packages("staplr")}

# Don't edit anything in this function
prep_for_crowdmark <- function(pdf=NULL){
  # Get the name of the file you're currently in. 
  this_file <- rstudioapi::getSourceEditorContext()$path
  pdf_name <- sub(".Rmd", ".pdf", sub('.*/', '', this_file))
  
  # Create a file called to_submit to put the individual files in
  # This will be in the same folder as this file is saved
  if(!match("to_submit", list.files(), nomatch = FALSE))
    {dir.create("to_submit")}
 
  # Split the files
  if(is.null(pdf)){
  staplr::split_pdf(pdf_name, output_directory = "to_submit", prefix = "page_")} else {
    staplr::split_pdf(pdf, output_directory = "to_submit", prefix = "page_") 
  }
}

prep_for_crowdmark()

```